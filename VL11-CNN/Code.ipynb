{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VL 11: CNNs\n",
    "\n",
    "This Jupyter Notebook demonstrates the process of loading, preprocessing, and modeling mortality rate data using a deep neural network.\n",
    "\n",
    "## Data Loading and Preprocessing\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "    - We import necessary libraries including `numpy`, `pandas`, and various modules from `tensorflow.keras` for building the neural network.\n",
    "    - We also import `LabelEncoder` from `sklearn.preprocessing` for encoding categorical data.\n",
    "\n",
    "2. **Loading Data**:\n",
    "    - We load the mortality rates data from a CSV file into a pandas DataFrame named `TrainData`.\n",
    "\n",
    "3. **Extracting Features and Target**:\n",
    "    - We extract the `Year`, `Age`, `Country`, and `sex` columns as features and the `log_mx` column as the target variable.\n",
    "    - We reshape the extracted features to be used in the neural network.\n",
    "\n",
    "4. **Encoding Categorical Data**:\n",
    "    - We use `LabelEncoder` to encode the `Country` and `sex` columns into numerical values.\n",
    "\n",
    "5. **Setting Limits**:\n",
    "    - We determine the maximum values for `Age`, `Country`, and `Gender` to set the input dimensions for the embedding layers.\n",
    "\n",
    "## Model Building\n",
    "\n",
    "1. **Defining Inputs**:\n",
    "    - We define the input layers for the rates, country, and gender.\n",
    "\n",
    "2. **Embedding Layers**:\n",
    "    - We create embedding layers for the `Country` and `Gender` inputs to convert them into dense vectors.\n",
    "\n",
    "3. **Convolutional Layers**:\n",
    "    - We apply a series of convolutional, pooling, batch normalization, and dropout layers to the rates input to extract features.\n",
    "\n",
    "4. **Concatenation and Dense Layers**:\n",
    "    - We concatenate the outputs of the convolutional layers and the embedding layers.\n",
    "    - We apply dropout and dense layers to the concatenated output to generate the final forecast rates.\n",
    "\n",
    "5. **Compiling the Model**:\n",
    "    - We compile the model using the Adam optimizer and mean squared error loss function.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "1. **Early Stopping**:\n",
    "    - We add an early stopping callback to prevent overfitting by monitoring the validation loss.\n",
    "\n",
    "2. **Generating Dummy Data**:\n",
    "    - We generate dummy data for the rates input to simulate the training process.\n",
    "    - **This is the part I don't understand a bit: What good does that do, why do I need it?**\n",
    "\n",
    "3. **Fitting the Model**:\n",
    "    - We fit the model on the training data with a validation split and the early stopping callback.\n",
    "\n",
    "This workflow provides a comprehensive approach to building and training a deep neural network for forecasting mortality rates based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Dropout, Dense, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age limit:  101 Country limit 42 Gender limit 2\n"
     ]
    }
   ],
   "source": [
    "TrainData = pd.read_csv('../VL10-DeepNeuralNetworks/mortality_rates.csv')\n",
    "YearData = TrainData['Year'].values.reshape(-1, 1)\n",
    "AgeData = TrainData['Age'].values.reshape(-1, 1)\n",
    "_CountryData = TrainData['Country'].values\n",
    "_GenderData = TrainData[\"sex\"].values\n",
    "# Encode country strings to integers\n",
    "label_encoder = LabelEncoder()\n",
    "CountryData = label_encoder.fit_transform(_CountryData).reshape(-1, 1)\n",
    "GenderData = label_encoder.fit_transform(_GenderData).reshape(-1, 1)\n",
    "\n",
    "ageLim = AgeData.max()+1\n",
    "CountryLim = CountryData.max()+1\n",
    "GenderLim = GenderData.max()+1\n",
    "\n",
    "print(\"Age limit: \", ageLim, \"Country limit\", CountryLim, \"Gender limit\", GenderLim) #I had to modify the code as I seemed to have a country more than the Wuethrich dataset. Gender and age however are the same.\n",
    "\n",
    "TargetData = TrainData['log_mx'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m9782/9782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1ms/step - loss: 29.4749 - val_loss: 28.6259\n",
      "Epoch 2/10\n",
      "\u001b[1m9782/9782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 29.3851 - val_loss: 28.6259\n",
      "Epoch 3/10\n",
      "\u001b[1m9782/9782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 29.4172 - val_loss: 28.6259\n",
      "Epoch 4/10\n",
      "\u001b[1m9782/9782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 29.3692 - val_loss: 28.6259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3736cdfd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the inputs\n",
    "rates = Input(shape=(10, 100), dtype='float32', name='rates')\n",
    "Country = Input(shape=(1,), dtype='int32', name='Country')\n",
    "Gender = Input(shape=(1,), dtype='int32', name='Gender')\n",
    "\n",
    "# Embedding layers\n",
    "Country_embed = Embedding(input_dim=CountryLim, output_dim=5)(Country)\n",
    "Country_embed = Flatten(name='Country_embed')(Country_embed)\n",
    "\n",
    "Gender_embed = Embedding(input_dim=GenderLim, output_dim=5)(Gender)\n",
    "Gender_embed = Flatten(name='Gender_embed')(Gender_embed)\n",
    "\n",
    "# Convolutional layers\n",
    "conv = Conv1D(filters=32, kernel_size=3, activation='linear', padding='causal')(rates)\n",
    "conv = MaxPooling1D(pool_size=2)(conv)\n",
    "conv = BatchNormalization()(conv)\n",
    "conv = Dropout(rate=0.35)(conv)\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "# Concatenate and dense layers\n",
    "decoded = Concatenate()([conv, Country_embed, Gender_embed])\n",
    "decoded = Dropout(rate=0.4)(decoded)\n",
    "decoded = Dense(units=100, activation='sigmoid')(decoded)\n",
    "decoded = Reshape((1, 100), name='forecast_rates')(decoded)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[rates, Country, Gender], outputs=decoded)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Generate dummy data for rates\n",
    "rates_data = np.random.rand(len(CountryData), 10, 100).astype('float32')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(\n",
    "    [rates_data, CountryData, GenderData], \n",
    "    TargetData, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
